# -*- coding: utf-8 -*-
"""classify _proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKZodyIqEU9B4g7JgOZkZOPJqASuQ1mB
"""

!pip install transformers -U

!pip install transformers[torch]

from google.colab import drive
drive.mount('/content/drive')

# !unzip /content/drive/MyDrive/qwe_1.zip -d /content

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns
import re
import pandas as pd
import time
import os

"""## Load training and test data"""

train_df = pd.read_csv('/content/drive/MyDrive/final_train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/final_valid_data.csv')

!pip install deep-translator

"""**yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy chatgpt

"""

id_num=0
columns = ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
filename = '/content/drive/MyDrive/base_gpt.csv'

if os.path.isfile(filename):
    print(f"CSV file '{filename}' already exists. Loading the existing file.")
    gpt_df = pd.read_csv(filename)
    print(gpt_df.index)
    id_num=gpt_df.index[-1]+1

else:
    gpt_df = pd.DataFrame(columns=columns)
    id_num=0
    gpt_df.to_csv(filename, index=False)
    print(f"CSV file '{filename}' has been created.",type(gpt_df))

!pip install h5py
!pip install typing-extensions
!pip install wheel

pip install openai==0.28.1

import openai
import time
import pandas as pd
openai.api_key = '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'
def get_answer(comment_text):
  try:
    messages = [ {"role": "system", "content":
              "You are a intelligent assistant."} ]
    # message = f"در متن زیر کدام یک از احساسات روبه‌رو وجود دارد؟ 1-تعجب 2-ناراحتی 3-تنفر 4-خوشحالی 5-ترس 6- عصبانیت 7- هیچکدام\n{tweet}"
    # message = f"review text below and say which one of front side items have ?.please answer summerize and if anyone does not contain in text, just say 'does contain', otherwise just to name parameters that exist and do not say another parameters .\n items[ toxic, severe_toxic, obscene, threat, insult, identity_hate]\ntext:\n{comment_text}"
    message = f"Review the text below and say which of the opposites is there in the text? Please answer briefly and if the text does not have any of the items, just say <none>, otherwise just name the items that are present and avoid mentioning the non-existent items.\n items[ toxic, severe_toxic, obscene, threat, insult, identity_hate]\ntext:\n{comment_text}"

    messages.append(
        {"role": "user", "content": message},
    )
    chat = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-0613", messages=messages
    )
    reply = chat.choices[0].message.content
    return True, reply
  except Exception as e:
    return False, e

labels = {
    'comment_text': ' ',
    'toxic': 0,
    'severe_toxic': 0,
    'obscene': 0,
    'threat': 0,
    'insult': 0,
    'identity_hate': 0
}
filegpt="/content/drive/MyDrive/base_gpt.csv"
tmp_df = pd.DataFrame(columns=columns)
count=0
id_num=0
while id_num < lengths:
    row=datas.iloc[id_num]
    comment_text=row['comment_text']
    labels = {'comment_text':comment_text,'toxic': 0,'severe_toxic': 0,'obscene': 0,'threat': 0,'insult': 0,'identity_hate': 0}
    flag,ans=get_answer(comment_text)
    print("ans",ans)
    if(flag==True):
      if (('none' in ans) or ('<none>' in ans)):
        id_num+=1
        tmp_df = tmp_df.append(labels, ignore_index=True)
        print(f"gpt answer row {id_num+1} successfully.")
        if((id_num)%100==0):
          gpt_df = pd.concat([gpt_df,tmp_df],ignore_index=True)
          gpt_df.to_csv(filegpt)
          tmp_df = pd.DataFrame(columns=columns)
        continue
      if ' toxic,' in ans:
        labels['toxic']=1
      if 'severe_toxic' in ans:
        labels['severe_toxic']=1
      if 'obscene' in ans:
        labels['obscene']=1
      if 'threat' in ans:
        labels['threat']=1
      if 'insult' in ans:
        labels['insult']=1
      if 'identity_hate' in ans:
        labels['identity_hate']=1

      id_num+=1
      tmp_df = tmp_df.append(labels, ignore_index=True)
      print(f"gpt answer row {id_num+1} successfully.")
      if(id_num%100==0):
        gpt_df=pd.concat([gpt_df,tmp_df], ignore_index=True)
        gpt_df.to_csv(filename, index=None)
        tmp_df = pd.DataFrame(columns=columns)
    else:
      count+=1
      if count>10:
        print("ooooooooooooooooooooooooooooooooooops")
        break;

gpt_df.to_csv(filename)

gpt_df

# cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']

"""eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeennnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddchat gpt

**start translate text**
"""

import os
translator = Translator()
filename = '/content/drive/MyDrive/trans_file.csv'
i=0
# Check if the file exists
if os.path.isfile(filename):
    df = pd.read_csv(filename)
    i=df.index[-1]+1

else:
    df = pd.DataFrame()
    i=0
print(i)

from deep_translator import GoogleTranslator
import os
translator = GoogleTranslator(source='en', target='fa')
filename ='/content/drive/MyDrive/transversion1_file.csv'
i=0
# Check if the file exists
if os.path.isfile(filename):
    df = pd.read_csv(filename)
    i=df.index[-1]+1

else:
    df = pd.DataFrame(columns=['comment_text'])
    i=0

# trans_file=pd.read_csv(filename)

counter=0
dataset={'toxic':[],'severe_toxic': [],'obscene':[],'threat':[],'insult':[],'identity_hate':[],'comment_trans_text':[],'comment_text':[]}
lengths=len(train_df.index)

while i<lengths:
  row=train_df.iloc[i]
  try:
      print(i,lengths,"you  never walk alone")
      if(i%500==0):
        df = pd.concat([df, pd.DataFrame(dataset,columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate','comment_trans_text','comment_text'])])
        df.to_csv(filename, index=None)
        dataset={'toxic':[],'severe_toxic': [],'obscene':[],'threat':[],'insult':[],'identity_hate':[],'comment_trans_text':[],'comment_text':[]}
        print("data saved in dataset file in my csv dataset file . hello world. you never walk alone")

      comment = row['comment_text']
      if(len(comment)>4990):
        comment=comment[:4900]
      # time.sleep(0.5)
      translated_text = translator.translate(comment)
      print(translated_text)
      counter=0
      dataset['comment_trans_text'].append(translated_text)
      dataset['comment_text'].append(comment)
      dataset['toxic'].append(row['toxic'])
      dataset['severe_toxic'].append(row['severe_toxic'])
      dataset['obscene'].append(row['obscene'])
      dataset['threat'].append(row['threat'])
      dataset['insult'].append(row['insult'])
      dataset['identity_hate'].append(row['identity_hate'])
      print(f"Translated row {i+1} successfully.")
      i+=1
      if( i==(lengths)):
        df = pd.concat([df, pd.DataFrame(dataset,columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate','comment_trans_text','comment_text'])])
        df.to_csv(filename, index=None)
        dataset={'toxic':[],'severe_toxic': [],'obscene':[],'threat':[],'insult':[],'identity_hate':[],'comment_trans_text':[],'comment_text':[]}
        print("data saved in dataset file in my csv dataset file . hello world. you never walk alone")
  except Exception as e:
      counter+=1
      if(counter>=10):
        break;
      print(f"Translation failed for row {i+1}. Error: {str(e)}")

"""**sperated train and validation data**"""

filename ='/content/drive/MyDrive/transversion1_file.csv'
dataset=pd.read_csv(filename)
# Shuffling with a random state for reproducibility
data_shuffled = dataset.sample(frac=1, random_state=42)

# Calculate the size of the validation set (10%)
validation_size = int(0.1 * len(data_shuffled))

# Split into validation and training sets
validation_data = data_shuffled[:validation_size]
train_data = data_shuffled[validation_size:]

# Save the validation and training sets to separate CSV files
validation_data.to_csv('/content/drive/MyDrive/final_valid_data.csv', index=False)
train_data.to_csv('/content/drive/MyDrive/final_train_data.csv', index=False)

pd.concat([df, pd.DataFrame(dataset)]).to_csv(filename)

translated_df = pd.read_csv(filename)

"""**end of tranlsate data**

## Examine the data (EDA)
"""

train_copy_df=train_df.copy()

cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']

# check missing values in numeric columns
# train_df.describe()

unlabelled_in_all = train_df[(train_df['toxic']!=1) & (train_df['severe_toxic']!=1) & (train_df['obscene']!=1) &
                            (train_df['threat']!=1) & (train_df['insult']!=1) & (train_df['identity_hate']!=1)]
print('Percentage of unlabelled comments is ', len(unlabelled_in_all)/len(train_df)*100)

# check for any 'null' comment
no_comment = train_df[train_df['comment_text'].isnull()]
len(no_comment)

no_comment = test_df[test_df['comment_text'].isnull()]
no_comment

# let's see the total rows in train, test data and the numbers for the various categories
print('Total rows in test is {}'.format(len(test_df)))
print('Total rows in train is {}'.format(len(train_df)))
print(train_df[cols_target].sum())

data = train_df[cols_target]

colormap = plt.cm.plasma
plt.figure(figsize=(7,7))
plt.title('Correlation of features & targets',y=1.05,size=14)
sns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,
           linecolor='white',annot=True)

"""## Clean up the comment text"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = text.strip(' ')
    return text

val_df=test_df

# clean the comment_text in train_df [Thanks to Pulkit Jha for the useful pointer.]
train_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))

val_df['comment_text'] = val_df['comment_text'].map(lambda com : clean_text(com))

# clean the comment_text in test_df [Thanks, Pulkit Jha.]
test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))

"""
## Define X from entire train & test data for use in tokenization by Vectorizer"""

X = train_df.comment_trans_text
test_X = test_df.comment_trans_text
x_val=val_df.comment_trans_text

print(X.shape, test_X.shape)

"""## Vectorize the data"""

!pip install datasets

import pandas as pd
from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

import datasets
batch_size =32
num_batches = len(train_df) // batch_size
tokenizer = AutoTokenizer.from_pretrained("HooshvareLab/roberta-fa-zwnj-base")

model = BertForSequenceClassification.from_pretrained("HooshvareLab/roberta-fa-zwnj-base",num_labels=6,problem_type="multi_label_classification").to('cuda')
# model = BertForSequenceClassification.from_pretrained("HooshvareLab/roberta-fa-zwnj-base",num_labels=6,problem_type="multi_label_classification")

!pip freeze | grep -i "accel"

pip install evaluate

import evaluate
from datasets import load_metric
def compute_metrics(eval_preds):
    metric = load_metric("accuracy")
    precision_metric = load_metric("precision")
    recall_metric = load_metric("recall")
    f1_metric = load_metric("f1")

    logits, labels = eval_preds
    predictions = (logits > 0.5).astype(dtype=int)
    labels = np.array(labels)
    result_accuracy = metric.compute(predictions=predictions.flatten(), references=labels.flatten())
    result_precision = precision_metric.compute(predictions=predictions.flatten(), references=labels.flatten())
    result_recall = recall_metric.compute(predictions=predictions.flatten(), references=labels.flatten())
    result_f1 = f1_metric.compute(predictions=predictions.flatten(), references=labels.flatten())
    return {"accuracy": result_accuracy, "precision": result_precision, "recall": result_recall, "f1": result_f1}

# start_idx = 0
# end_idx =  1 * 10
# subset = train_df.iloc[start_idx:end_idx]
# tokenized_batch = tokenizer(subset['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")
# train_dataset = Dataset.from_dict({
#     "input_ids": tokenized_batch["input_ids"],
#     "attention_mask": tokenized_batch["attention_mask"],
#     "labels": subset[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
# })
# output_dir = "/content/drive/MyDrive/temp"
# training_args = TrainingArguments(
#     output_dir=output_dir,
#     num_train_epochs=1,
#     per_device_train_batch_size=8,
#     warmup_steps=1,
#     weight_decay=0.01,
#     logging_dir=output_dir + "/logs",
#     load_best_model_at_end=True,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",

# )
# val_subset = val_df.iloc[0:10]
# val_encodings = tokenizer(val_subset['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")
# val_dataset = Dataset.from_dict({
#   "input_ids": val_encodings["input_ids"],
#   "attention_mask": val_encodings["attention_mask"],
#   "labels": val_subset[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
# })
# eval_1=evaluate.load("accuracy")
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     compute_metrics = compute_metrics
#     )
# trainer.train()
# results = trainer.evaluate(eval_dataset=val_dataset)
# accuracy = results['eval_accuracy']
# print(f"Accuracy: {accuracy}")

# accuracy = results['eval_accuracy']
# precision = results['eval_precision']
# recall = results['eval_recall']
# f1 = results['eval_f1']
# print(f"Accuracy: {accuracy}")
# print(f"Precision: {precision}")
# print(f"Recall: {recall}")
# print(f"F1 Score: {f1}")

# del tokenized_batch
# del train_dataset

tokenized_batch = tokenizer(train_df['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")

train_dataset = Dataset.from_dict({
    "input_ids": tokenized_batch["input_ids"],
    "attention_mask": tokenized_batch["attention_mask"],
    "labels": train_df[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
})

output_dir = "/content/drive/MyDrive/final_roberta"
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    # gradient_accumulation_steps=2,
    warmup_steps=1,
    weight_decay=0.01,
    logging_dir=output_dir + "/logs",
    load_best_model_at_end=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
)

val_encodings = tokenizer(val_df['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")
val_dataset = Dataset.from_dict({
  "input_ids": val_encodings["input_ids"],
  "attention_mask": val_encodings["attention_mask"],
  "labels": val_df[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
})

# tokenized_batch = tokenizer(train_df['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")
# train_dataset = Dataset.from_dict({
#     "input_ids": tokenized_batch["input_ids"],
#     "attention_mask": tokenized_batch["attention_mask"],
#     "labels": train_df[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
# })
# output_dir = "/content/drive/MyDrive/final_roberta"
# training_args = TrainingArguments(
#     output_dir=output_dir,
#     num_train_epochs=3,
#     per_device_train_batch_size=4,
#     # gradient_accumulation_steps=2,
#     warmup_steps=1,
#     weight_decay=0.01,
#     logging_dir=output_dir + "/logs",
#     load_best_model_at_end=True,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
# )
# val_encodings = tokenizer(val_df['comment_trans_text'].tolist(), padding=True, truncation=True, return_tensors="pt")
# val_dataset = Dataset.from_dict({
#   "input_ids": val_encodings["input_ids"],
#   "attention_mask": val_encodings["attention_mask"],
#   "labels": val_df[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
# })


# del tokenized_batch  # If no longer needed
# del val_encodings  # If no longer needed


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics = compute_metrics
    )
trainer.train()

results = trainer.evaluate(eval_dataset=val_dataset)
accuracy = results['eval_accuracy']
precision = results['eval_precision']
recall = results['eval_recall']
f1 = results['eval_f1']
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

train_dataset = Dataset.from_dict({
    "input_ids": tokenized_train["input_ids"],
    "attention_mask": tokenized_train["attention_mask"],
    "labels": train_df[['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']].values.tolist(),
})
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=6)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
trainer.train()

"""end task format
**end task format**
"""

# persian_stop_words = np.loadtxt('/content/drive/MyDrive/stop_words.txt', dtype=str, delimiter='\n')
# # vect = CountVectorizer(stop_words=persian_stop_words)

# # import and instantiate TfidfVectorizer
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.feature_extraction.text import TfidfVectorizer
# vect = TfidfVectorizer(max_features=5000,stop_words='english')
# vect

# learn the vocabulary in the training data, then use it to create a document-term matrix
X_dtm = vect.fit_transform(X)
# examine the document-term matrix created from X_train
X_dtm

val_x_dtm = vect.transform(x_val)
val_x_dtm

# transform the test data using the earlier fitted vocabulary, into a document-term matrix
test_X_dtm = vect.transform(test_X)
# examine the document-term matrix from X_test
test_X_dtm

"""## Binary Relevance - build a multi-label classifier using Logistic Regression"""

test_label=pd.read_csv('./dataset/test_labels.csv')
pd.merge(test_df,test_label)

# import and instantiate the Logistic Regression model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
logreg = LogisticRegression(C=12.0,max_iter=1000)

# create submission file
# submission_binary = pd.read_csv('./dataset/sample_submission.csv')

for label in cols_target:
    print('... Processing {}'.format(label))
    y = train_df[label]
    y_val=val_df[label]
    # train the model using X_dtm & y
    logreg.fit(X_dtm, y)
    # compute the training accuracy
    y_pred_X = logreg.predict(X_dtm)
    print('Training: accuracy = {} ,precision={}, recall={} ,f1_score={}'.format(accuracy_score(y, y_pred_X),precision_score(y, y_pred_X),
                                                                               recall_score(y, y_pred_X),f1_score(y, y_pred_X)))

    y_pred_val=logreg.predict(val_x_dtm)
    print('validation: accuracy = {} ,precision={}, recall={} ,f1_score={}'.format(accuracy_score(y_val, y_pred_val),precision_score(y_val, y_pred_val),
                                                                               recall_score(y_val, y_pred_val),f1_score(y_val, y_pred_val)))
    # compute the predicted probabilities for X_test_dtm

    # test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]
    # submission_binary[label] = test_y_prob

train_df

pip install pandas google-cloud-translate

import pandas as pd
from google.cloud import translate

# Set up authentication with your credentials JSON file
translate_client = translate.TranslationServiceClient.from_service_account_json('path/to/credentials.json')

# Read the CSV file into a pandas DataFrame
df = pd.read_csv('your_file.csv')

# Specify the column containing the texts to be translated
column_to_translate = 'column_name'  # Replace with the actual column name

# Translate the texts in the specified column
translated_texts = []
for text in df[column_to_translate]:
    response = translate_client.translate_text(
        parent='projects/your-project-id/locations/global',
        contents=[text],
        mime_type='text/plain',
        target_language_code='es',  # Replace with the desired target language code
    )
    translated_text = response.translations[0].translated_text
    translated_texts.append(translated_text)

# Add the translated texts as a new column in the DataFrame
df['translated_column'] = translated_texts

# Save the DataFrame to a new CSV file
df.to_csv('translated_file.csv', index=False)